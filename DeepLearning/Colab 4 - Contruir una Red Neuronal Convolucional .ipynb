{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <font color=gray>Este dataset __cifar10__ es un conjunto de 60.000 imagenes de 32X32 usado para el Computer Vision con 10 clases de objetos a color el cual cada uno contiene 6.000 imgs. [https://www.kaggle.com/c/cifar-10/data]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 1 & 2 Instalación e Importación dedependencias\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "%matplotlib inline\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3 : Pre procesado de datos\n",
    "# Configurar el nombre de las clases del dataset\n",
    "class_names = ['avión', 'coche', 'pájaro', 'gato', 'ciervo', 'perro', 'rana', 'caballo', 'barco', 'camión']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 192s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tridimensional\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1eed0920148>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4 Construir una red neuronal convolucional\n",
    "# definir el modelo como instancia de la clase Sequential\n",
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyper parámetros de la capa de la RNC:**\n",
    "\n",
    "    Filtros: 32\n",
    "    Tamaño del kernel: 3\n",
    "    padding: same\n",
    "    Función de Activación: relu\n",
    "    input_shape: (32, 32, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color=gray>Capa convolucional, **32 filtros**, con **3 puntos horizontal y vertical**\n",
    "El **kernel** nos permite hacer la media, la ponderación la convolución o producto convolucional del quernel con la capa\n",
    "de la imagen, **padding** es el margen con respecto a los bordes para que el filtro pase por las pocisiones y tome mas o\n",
    "menos el mismo valor => [https://www.pico.net/kb/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-tensorflow]. **En el input shape** ya no se recibe un vector son imagenes de 32X32 por 3 canales de color (RGB)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Capa 1 : Input\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=[32, 32, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyper parámetros de la capa de la RNC:**\n",
    "\n",
    "    Filtros: 32\n",
    "    Tamaño del kernel: 3\n",
    "    padding: same\n",
    "    Función de Activación: relu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyper parámetros de la capa de MaxPool:**\n",
    "\n",
    "    pool_size: 2\n",
    "    strides: 2\n",
    "    padding: valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray>MaxPooling moviendo 2 pocisiones en horizontal y vertical en una ventana, se va a quedar con el valor mas alto, despues de aplicar la convolución, el Maxpooling reducira el tamaño de la imagen. **IMPORTANTE** En consecuencia de como se comporta el paddin en la Conv2d (tomando un conjunto de 0roz)  es posible que se pierdan datos allí, por eso MaxPool2D tomara los valores (los rasgos mas may) para la convolución **por medio de un padding valido** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capa 2 : Capa convolucional Conv2D de padding same y la Capa convolucional MaxPool2D padding valido\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyper parámetros de la capa de la RNC:**\n",
    "\n",
    "    Filtros: 64\n",
    "    Tamaño del kernel: 3\n",
    "    padding: same\n",
    "    Función de Activación: relu (Rectificadora lineal unitaria)\n",
    " \n",
    "Argumentos:\n",
    "\n",
    "    pool_size: entero o tupla de 2 enteros, factores por los cuales reducir la escala (vertical, horizontal). (2, 2) reducirá a la mitad la entrada en ambas dimensiones espaciales. Si solo se especifica un número entero, se utilizará la misma longitud de ventana para ambas dimensiones.\n",
    "    zancadas: entero, tupla de 2 enteros o ninguno. Strides valores. Si ninguno, por defecto será pool_size.\n",
    "    relleno: uno de 'válido' o 'igual' (no distingue entre mayúsculas y minúsculas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capa 3 :  Capa convolucional con el doble de filtros\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Añadir la cuarta capa convolucional y la capa de max-pooling**\n",
    "\n",
    "Hyper parámetros de la capa de la RNC:\n",
    "\n",
    "    Filtros: 64\n",
    "    Tamaño del kernel: 3\n",
    "    padding: same\n",
    "    Función de Activación: relu\n",
    "\n",
    "Hyper parámetros de la capa de la MaxPool:\n",
    "\n",
    "    pool_size: 2\n",
    "    strides: 2\n",
    "    padding: valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capa 4 : Capa convolucional Conv2D de padding same y la Capa convolucional MaxPool2D padding valido\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Añadir la capa de flattening**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray>Sigue siendo una imagen 2d con tres canales de color, por eso FLatten() aplana las imagenes, las convierte en un vector de datos aplanados, una vez que han sido filtrados y reducidos, esta sera la capa de entrada de una red neuronal **totalmente conectada por tanto la fase de convolución es un añadido que se le coloca a una red neuronal artificial antes de la entrada, para contruir a partir de las imagenes sus convoluciones y aplanar ese resultado para que sea la entrada.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Añadir la primera capa fully-connected**\n",
    "\n",
    "Hyper parámetros de la capa totalmente conectada:\n",
    "\n",
    "    units/neurons: 128\n",
    "    activation: relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capa 5 : RNA totalmente conectada \n",
    "model.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Añadir la capa de salida**\n",
    "\n",
    "Hyper parámetros de la capa totalmente conectada:\n",
    "\n",
    "    units/neurons: 10 (number of classes)\n",
    "    activation: softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 591,274\n",
      "Trainable params: 591,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray>Compilar el modelos arse_categorical_accuracy comprueba si el valor verdadero maximal coincide con el índice maximal del valor de la predicción. El **optimizador** es la herramienta que nos permite llevar a cabo el gradiende de descenso estocastico, que actualiza los pesos cada vez que hace una propagación hacia atras. Que ademas reduce la perdida: es  el error entre la predicción del modelo y el dato original.\n",
    "https://stackoverflow.com/questions/44477489/keras-difference-between-categorical-accuracy-and-sparse-categorical-accuracy</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comilando el modelo.\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['sparse_categorical_accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 145s 3ms/sample - loss: 1.3579 - sparse_categorical_accuracy: 0.5115\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 147s 3ms/sample - loss: 0.8992 - sparse_categorical_accuracy: 0.6850\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 145s 3ms/sample - loss: 0.7324 - sparse_categorical_accuracy: 0.7452\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 150s 3ms/sample - loss: 0.6173 - sparse_categorical_accuracy: 0.7832\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 151s 3ms/sample - loss: 0.5106 - sparse_categorical_accuracy: 0.8206s - loss: 0.5105 - sparse_categorical_accuracy - ETA: 4s - loss:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eed0b8b608>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 657us/sample - loss: 0.7694 - sparse_categorical_accuracy: 0.7420\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7419999837875366\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
