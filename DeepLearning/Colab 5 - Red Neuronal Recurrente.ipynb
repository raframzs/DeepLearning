{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray>[https://www.kaggle.com/mantri7/imdb-movie-reviews-dataset] Es la calificación de un set de peliculas donde en la escala de 1 a 10 => 5>positiva o menor que 5 negativa, al final sera una clasificación binaria, en formato de texto, __toca predecir si la valoración que se esta dando es positiva o negativa, solo subministrandole el texto__. Tiene 25K para entrenar y 25k para test</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Internet Movie Data Base\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-prosesado de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray>__Se le parasaran valoraciones de la misma longitud, es un requisito que todas tengan la misma longitud para empezar, en tal caso se añadiran palabras adicionales para que todas queden con la misma longitud___</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__number_of_words__\n",
    "    * Vocavulario: Cuando importemos las valoraciones de las peliculas vamos a quedarnos con aquellas que tengan las palabras mas frecuentes, el objetivo es quedarno con las valoraciones que tienen sentido.\n",
    "__max_long__\n",
    "    * Si tenemos una valoración con 5 palabras, esta se va a representar como una valoración de 100 elementos donde los 5 primeros seran el contenido Original y los otros 95 una misma palabra repetida hasta completar 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar parámetros del dataset\n",
    "number_of_words = 20000\n",
    "max_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del data set de IMDB\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cortar Secuencias de texto de la misma longitud para que todas queden de 100 elementos\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen = max_len )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = number_of_words\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Construir la Red Neuronal Recurrente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo\n",
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadir la capa de embedding (incrustar, añadir, colocar). Toma los textos antes de ser subministrados a la red neuronal. __Esta capa__ crea un vector de palabras traduciendolo a una matriz de 0roz y 1nos, Esto se hace con la funcion de Embedding se utiliza para entrenar grandes matrizes o vectores y tranformarla en una gran matriz, __donde cada fila se corresponde con una de las valoraciones__ y las columnas son precizamente las palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parametros__\n",
    "    * vocab_size => 20 mil potenciales palabras\n",
    "    * embed_size => 128 pesos para cada palabra que se este analizando.\n",
    "    * input_shape => Se indica el tamaño de entrada (todas las valoraciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir la capa de embedding\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, embed_size, input_shape = (X_train.shape[1],)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Añadir la capa de LSTM__\n",
    "    * unidades: 128\n",
    "    * función de activación: tanh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> https://en.wikipedia.org/wiki/Long_short-term_memory </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.LSTM(units = 128, activation = 'tanh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray>__Dense__ capa totalmente contectada, que dara como respuesta una neurona que dara la valoración positiva o negativa y ademas hara uso de la funcion __sigmoid__ que me transformara el resultado anterior en una probabilidad entre [0,1]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir la capa totalmente conectada de salida\n",
    "model.add(tf.keras.layers.Dense(units = 1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parametros__\n",
    "    * 'rmsprop' => la proporción del error cuadrado medio (es el recomendado para RNR)\n",
    "    * 'binary_crossentropy' => pues la operación es binaria \n",
    "    * metrica =>  asociada es accuracy o es cierto o es falso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 20.000 columnas * 128 neuronas (pesos)\n",
    "# LSTM filtrado con la tangente hyperbolica \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 45s 2ms/sample - loss: 0.2335 - accuracy: 0.9108\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.1882 - accuracy: 0.9300\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 47s 2ms/sample - loss: 0.1572 - accuracy: 0.9436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12484080cc8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 25s 1000us/sample - loss: 0.3949 - accuracy: 0.8478\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acurracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8478400111198425\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy: {}\".format(test_acurracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
